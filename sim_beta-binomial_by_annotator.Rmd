---
title: "Fitting Carpenter's Beta-Binomial by Annotator model to simulated data using JAGS."
author: "Hauke Licht"
output: html_document
---

```{r knitr, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = 'center', fig.width = 7, fig.height = 4, out.width = "90%", out.height = "90%")
```

## Research Interest 

I want to measure the salience of three different dimensions of populism in textual data:

- *people-centrism*: a statement praises 'the people' as virtuous, homogenous societal group, whose interests and opinions should be given priority in political decision making
- *anti-elitism*: a statement portrays 'the elites' as enemies of the people and puts into question this elites intentions and moral integrity, particular with regard to their role as representatives of the people and its interests.
- *exclusionism*: a statement portrays some societal group as outgroup of 'the people'

## Research Desing

I use data collect by Withney Hua, Tarik Abou-Chadi and Pablo Barberá.[^1]
They have downloaded social media posts authored by the Twitter and Facebook accounts used by party leaders and parties in a selected set of Western European countries.
Then they recruited crowd workers via a crowd-sourcing platform and asked them to judge a given post along each of the above-mentioned dimensions of populism.
This data collection effort has yielded a total 476 judgments on each dimensions generated by three different coders for 185 statements.

[^1]: Hua, Whitney, Tarik Abou-Chadi, and Pablo Barberá (2018). "Networked Populism: Characterizing the Public Rhetoric of Populist Parties in Europe". Paper prepared for the 2018 MPSA Conference.

### The model

I assume that the above-defined dimensions are latent features of political statements, and hence crowd coders act as human content analysts whose judgments I want to aggregate at the statement-level to estimate whether a given statement belongs to either of these three categories.

For each dimension, the setup can be described as a four-tuple $\langle\mathcal{I}, \mathcal{J}, \mathcal{K}, \mathcal{Y}\rangle$, where

- $\mathcal{I}$ is the set of \emph{statements} $i \in 1,\ \ldots ,\ n$ distributed for crowd coding,
- $\mathcal{J}$ is the set of \emph{coders} $j \in 1,\ \ldots ,\ m$,
- $\mathcal{K}$ is the set of \emph{classes} $k \in \{0, 1\}$ defined by the categorical coding scheme used during crowd-coding, and
- $\mathcal{Y}$ is the set of \emph{judgments} (or codings) $y_{i,j} \in \{0, 1\}$ recorded for statement $i$ by coder $j$.

It is provided that $\mathcal{Y}$ contains at least one judgment per statement, that is, $|\mathcal{Y}_i| \geq 1\ \forall\ i \in \mathcal{I}$, and that $|\mathcal{Y}_i| \geq 2\ \forall\ i \in \mathcal{I}' \subset \mathcal{I}$. 
Moreover, judgments for each statement in $\mathcal{I}'$ are generated by different coders (i.e., we have repeated annotation at the statement level, but not at the judgment-statement level). 

Importantly, while coders' judgments of statements are observed, true class labels $l_i \in \mathcal{K}$ are unknown *a priori* for all $i = 1,\ \ldots,\ I$.
In this setup, a classification of items into classes obtained from a set of judgments $\rho(\mathcal{Y}) \Rightarrow \mathcal{L}$ is called a *'ground truth'* or *labeling*.

I want to fit a Bayesian *probabilistic annotation model* as described by Bob Carpenter[^2] in order to estimate the following parameters from the data recorded for the above setup:


$$
\begin{align*}
c_i &\sim\ \mbox{Bernoulli}(\pi)\\
\theta_{0j} &\sim\ \mbox{Beta}(\alpha_0 , \beta_0)\\
\theta_{1j} &\sim\ \mbox{Beta}(\alpha_1 , \beta_1)\\
y_{ij} &\sim\ \mbox{Bernoulli}(c_i\theta_{1j} + (1 - c_i)(1  - \theta_{0j}))\\
{}&{}\\
\pi &\sim\ \mbox{Beta}(1,1)\\
\alpha_0/(\alpha_0 + \beta_0) &\sim\ \mbox{Beta}(1,1)\\  
\alpha_0+\beta_0 &\sim\ \mbox{Pareto}(1.5)\\
\alpha_1/(\alpha_1 + \beta_1) &\sim\ \mbox{Beta}(1,1)\\ 
\alpha_1+\beta_1  &\sim\ \mbox{Pareto}(1.5)
\end{align*}
$$
where 

- $c_i$ is the 'true' (unobserved) class of statement $i$,
- $\pi$ is the 'true' prevalence of the positive class,
- $\theta_{0,j}$ is coder $j$'s specificity (true-negative rate),
and
- $\theta_{1,j}$ is her sensitivity (true-positive rate).

[^2]: Carpenter, Bob (2008). "Multilevel Bayesian Models of Categorical Data Annotation". unpublished manuscript.

This model can be graphically depicted as follows:

<center>
<img src="carpenter_2008_beta-binomial_by_annotator_plate_diagram.png" width="600"/>
</center>

## Implementation in **R** and **JAGS**

```{r setup}
# set digits
options(digits = 3)
# important: set seed for reproducability reasons
set.seed(1234)

# load required packages
library(rjags)
library(purrr)
library(dplyr)
library(tidyr)
library(extraDistr)
library(ggplot2)
library(ggridges)

#' Obtain parameter estimates from MCMC fit object
#' 
#' @descritpion Given an \code{mcmc.list} object created by fitting a JAGS model,
#'      and a list of parameters of interest, the function returns the 
#'      parameter estimates for each chain and all iterations in a tidy dataframe
#' 
#' @param fit.obj A \code{mcmc.list} containing the parameter estimates for all iterations and all chains obtained by fitting a JAGS model using \code[rjags]{}
#' 
#' @param params A character vector specifiying the parameters of interest.
#'    If \code{use.regex = TRUE} (the default), regular expressions will be expected.
#' 
#' @param use.regex Logical, specifying whether \code{params} is defined using regular expressions
#'    Defaulting to \code{TRUE}
#'
#' @return A tidy dataframe with columns
#'    'parameter' (the paramenter, character),
#'    'chain' (chain counter, integer),
#'    'est' (the estimate), and
#'    'iter' (iteration counter, integer)
#'
#' @note Functionality comparable to the \code[ggmcmc]{ggs} function in the \code[ggmcmc]{ggmcmc} pacakge.
#'
#' @importFrom tibble tibble
#' @importFrom purrr map_df
#' @importFrom dplyr mutate
#' @importFrom dplyr row_number
#' 
get_mcmc_estimates <- function(fit.obj, params, use.regex = TRUE){

  if (!inherits(fit.obj, "mcmc.list"))
    stop("`fit.obj` must be a 'mcmc.list' object")
  
  # test number of chains (equals No. top-level list elements)
  if ( (n_chains <- length(fit.obj)) == 0)
    stop("`fit.obj` has zero length")
  
  # get parameters contained in fit object
  these_params <- colnames(fit.obj[[1]])
  
  # get index positions of parameters to be obtained
  if (use.regex) {
    idxs <- which(grepl(params, these_params))
  } else {
    if (any((miss_params <- !params %in% these_params)))
      warning(
        "The following `params` are not contained in `fit.obj`: ", 
        paste0("'", params[miss_params], "'", collapse = ", ")
      )
    idxs <- which(these_params %in% params)
  }
  
  # obtain parameter estimates in tidy data frame
  suppressWarnings(
    purrr::map_df(these_params[idxs], function(param, obj = fit.obj){
      purrr::map_df(1:n_chains, function(c, o = obj, p = param) {
        tibble::tibble(
          parameter = param
          , chain = c
          , est = o[[c]][, p]
        ) %>% 
          dplyr::mutate(iter = dplyr::row_number())
      })
    })
  )
}

```

## Simulating the data-generation process

I follow Carpenter's original setup and simulated 20 annotators over 1000 items with a 50% missingness at random rate, and a prevalence of $\pi = 0.20$,.
```{r sim parameters 1}
# simulation parameters
n_items <- 1000
n_coders <- 20
missingness_rate <- .5

# data generation parameters
pi <- .2
alpha0 <- 40
beta0 <- 8
alpha1 <- 20
beta1 <- 8
```

Annotators' specificity and sensitivity parameters $\theta_{j0}, \theta_{j1}$ are drawn from $f_{\text{Beta}}(\alpha_0, \beta_0)$ with $(\alpha_0, \beta_0) = (40, 8)$ and
$f_{\text{Beta}}(\alpha_1, \beta_1)$ with $(\alpha_1, \beta_1) = (20, 8)$, respectively.

The probability density functions (PDFs) of the $\theta_{j\cdot}$ parameters look as follows: 

```{r plot annotator parameter PDFs, echo = FALSE}
# sampling distribution annotator-specific parameters
pdf_theta0 <- function(x) dbeta(x, alpha0, beta0)
pdf_theta1 <- function(x) dbeta(x, alpha1, beta1)

# par(mfrow = c(1,2))
# curve(pdf_theta0, from = 0, to = 1, ylab = "Density", xlab = expression(theta[0]), main = "Beta(40, 8)")
# curve(pdf_theta1, from = 0, to = 1, ylab = "Density", xlab = expression(theta[1]), main = "Beta(20, 8)")

ggplot(data.frame(x = seq(0,1,.1)), aes(x)) + 
  stat_function(fun=pdf_theta0, aes(color = "theta[0]")) +
  stat_function(fun=pdf_theta1, aes(color = "theta[1]")) +
  # stat_function(fun=pdf_theta0, aes(color = expression(theta[0]%~%Beta(40,8)))) +
  # stat_function(fun=pdf_theta1, aes(color = expression(theta[1]%~%Beta(20,8)))) +
  labs(
    title = expression(paste("Prior densities ", theta[0], " and ", theta[1]))
    , y = "Density"
    , x = expression(theta[.])
    , color = ""
  ) + 
  theme_bw() +
  theme(legend.position = "top") +
  scale_colour_discrete(
    labels = c(
      expression(theta[0]%~%f[Beta](40, 8))
      , expression(theta[1]%~%f[Beta](20, 8))
    )
  )
```

Given this setup, we first sample the 1000 instance from a Bernoulli disrtribution with p = .2, and coders' specificieties and sensitivities.
```{r sim drawing items}
# sample items' classes
sim_item_classes <- as.integer(rbernoulli(n = n_items, p = pi))

# sample coders' ability parameters
sim_theta0 <- rbeta(n = n_coders, shape1 = alpha0, shape2 = beta0)
sim_theta1 <- rbeta(n = n_coders, shape1 = alpha1, shape2 = beta0)
```

We can inspect the bivariate distribution of coders' 'true' ability parameters:
```{r plot sim thetas, echo = FALSE}
tibble(
  theta0 = sim_theta0
  , theta1 = sim_theta1
) %>% 
  ggplot(aes(x = theta0, y = theta1)) +
    geom_vline(aes(xintercept = mean(theta0)), color = "grey") +
    geom_hline(aes(yintercept = mean(theta1)), color = "grey") + 
    geom_point() +
    scale_x_continuous(limits = 0:1) +
    scale_y_continuous(limits = 0:1) +
    labs(
      title = "Distribution of coders' simulated ability parameters"
      , subtitle = expression(paste("Sensitivity (",theta[1],") vs. specificity (",theta[0],")"))
      , x = expression(theta[0])
      , y = expression(theta[1])
    ) +
    theme_bw() +
    theme(
      axis.title.y = element_text(angle = 0, vjust = .5) 
    )
```
They nicely scatter so that we have different coder types.[^3]

[^3]: That is, individual coders combine different capabilities ranging from low to high specificity as well as from low to high sensitivity, respectively.

Given these data, we can proceed to generate a set of codings.
Note that the below code does not implement the must efficient way to generte these data, but it is a very direct translation of the data generation process expressed in the model specification.
```{r sim codings}
# for each item i
sim_codings <- map_df(1:n_items, function(i){
  
  # sample ~10 coders who judge the item's class
  idxs <- sample(1:n_coders, ceiling(n_coders*(1-missingness_rate)))
  
  # for each coder in the sample, 
  y_i <- map_int(idxs, function(idx) {
    # generate coder j's judgment given item i' s 'true' class:
    #  - if the 'true' class is positive, j classifies it as positive with probability theta_{j1}
    #  - if the 'true' class is negative, j classifies it as negative with probability theta_{j0}
    rbernoulli(1, p = sim_item_classes[i]*sim_theta1[idx] + (1 - sim_item_classes[i])*(1 - sim_theta0[idx]) )
  })
  
  # gather data and return 
  tibble(
    # the item index (ID)
    item = i
    # coders indices (IDs)
    , coder = idxs
    # simulated judgments 
    , judgment = y_i
    # item i's 'true' class
    , true_class = sim_item_classes[i] 
  )
})

sim_codings
```

Given the simulated codings, we then first want to check whether the simulated data allows to reconstruct the imput parameters

```{r inspect sim}
# inspect coders' performance
coder_performances <- sim_codings %>% 
  group_by(coder) %>% 
  summarise(
    n_judgments = n_distinct(item)
    , accuracy = sum(judgment == true_class)/n_distinct(item)
    , tp = sum(judgment == 1 & true_class == 1)
    , fn = sum(judgment == 0 & true_class == 1)
    , fp = sum(judgment == 1 & true_class == 0)
    , tn = sum(judgment == 0 & true_class == 0)
    , TPR = tp / (tp + fn)
    , TNR = tn / (tn + fp) 
  ) %>% 
  arrange(desc(accuracy))
```

```{r plot coder performance, echo = FALSE, fig.width=7, fig.height=5, echo = FALSE}
coder_performances %>% 
  select(-matches("^[tfpn]{2}$")) %>% 
  gather("metric", "value", -coder, -n_judgments) %>% 
  ggplot(
    aes(
      x = value
      , y = coder
      , group = factor(metric)
      # , size = n_judgments
    )
  ) +
    geom_point(
      aes(size = n_judgments)
      , shape = 1
      , color = "grey"
      , alpha = .75
    ) +
    geom_point(
      shape = 20
      , size = .5
    ) +
    geom_point(
      shape = 3
      , data = tibble(
        coder = rep(1:n_coders, 3)
        , n_judgments = rep(3, n_coders*3)
        , value = c(rep(NA, n_coders), sim_theta0, sim_theta1)
        , metric = rep(c("accuracy", "TNR", "TPR"), each = n_coders)
      )
      , aes(y = coder, x = value, group = coder, color = "red")
    ) +
    scale_x_continuous(breaks = seq(0,1,.2)) + 
    scale_y_continuous(
      breaks = 1:20
      , limits = c(0.5, 20.5)
    ) + 
    scale_colour_discrete(guide = FALSE) +
    facet_grid(~factor(metric)) + 
    labs(
      title = "Observed coder performance in simulated data vs. simulated ability paramaters"
      # , subtitle = "Accuracies, true-negative rates (TNR), and true-positive rates (TPR)"
      , subtitle = "Red crosses mark simulated parameter values"
      , x = ""
      , y = "Coder"
      # , color = "Simulated parameter values"
      , size = "No. judgments"
      , caption = "
      accuracy = (true-positives + true-negatives) / N
      TFR = true-positives / (true-positives + false-negatives)
      TNR = true-negatives / (true-negatives + false-positives)"
    ) +
    theme_bw() +
    theme(
      plot.caption = element_text(hjust = 0)
      , legend.position = "top"
    )

```

We can see that the simulated, i.e., 'observed' coder-specific specificities and sensitivities fall in the ranges [`r range(coder_performances$TNR)`] and [`r range(coder_performances$TPR)`], respectively, and largely overlap the (randomly drawn) 'true' but unobserved parameter values.
Generally, simulated coders are fairly accurate, resulting in a fairly high corpus-level accuracy:
```{r inspect sim confusion}
# inspect confusion matrix
m_confusion <- sim_codings %>% 
  group_by(true_class, judgment) %>% 
  summarise(n = n()) %>% 
  arrange(-true_class, -judgment) %>% 
  ungroup() 

m_confusion

m_confusion %>%
  summarize(accuracy = sum(ifelse(true_class == judgment, n, 0))/sum(n))

```

## Fitting the beta-binomial by annotator model

### Model specification in JAGS

The complete JAGS code for the model reads as follows:
```
model{
    for (i in 1:N){
        c[i] ~ dbern(pi)
    }

    for (j in 1:M) {
        theta0[j]~ dbeta(alpha0, beta0);
        theta1[j]~ dbeta(alpha1, beta1);
    }

    for (j in 1:J) {
        y[j,3] ~ dbern(c[y[j,1]]*theta1[y[j,2]]+(1-c[y[j,1]])*(1-theta0[y[j,2]]))
    }

    pi ~ dbeta(1,1);
    
    mean0 ~ dbeta(1,1);
    scale0 ~ dpar(1.5,1);
    alpha0 <- mean0*scale0;
    beta0 <- scale0-alpha0;

    mean1 ~ dbeta(1,1);
    scale1 ~ dpar(1.5,1);
    alpha1 <- mean1*scale1;
    beta1 <- scale1-alpha1;
}
```

Let's walk step by step through the code:
We begin with modelling items' class membership as draws from a Bernoulli distribution governed by the hyperprior `pi`, the 'true' prevalence of the positive class.
```
for (i in 1:N){
    c[i] ~ dbern(pi)
}
```
The positive-class prevalence, in turn, follows a Beta distribution with mean and shape parameters $a = b = 1$. 
```
pi ~ dbeta(1,1)
```
This flat prior reflects our lack of knowledge regarding the prevalence of positive instances in the data.

Next, we draw coders' specificities and sensitivities, respectively, from two different Beta distributions that are intern governed by different sets of hyperpriors, (`alpha0`, `beta0`) and (`alpha1`, `beta1`), that are part of the model.
```
for (j in 1:M) {
    theta0[j]~ dbeta(alpha0, beta0);
    theta1[j]~ dbeta(alpha1, beta1);
}
```
The distribution of coder specificities is parametrezized in terms of the mean and scale of the Beta-distribution.
Specifically, we put a uniform prior on the mean $\alpha/(\alpha+\beta)$ and a uniform prior on the inverse square scale $1/(\alpha+\beta)^2$.
As Carpenter explicates, prior on the means is conveniently expressed using a Beta prior with $a = b = 1$, whereas the flat prior on the inverse square scale is expressed as a Pareto prior with $\alpha = 1.5, c=1$:

```
mean0 ~ dbeta(1,1);
scale0 ~ dpar(1.5,1);
alpha0 <- mean0*scale0;
beta0 <- scale0-alpha0;

mean1 ~ dbeta(1,1);
scale1 ~ dpar(1.5,1);
alpha1 <- mean1*scale1;
beta1 <- scale1-alpha1;
```
Let's have a look at how the mean and scale parameters are distributed
```{r plot hyperprior distr, echo = FALSE}
ggplot(data.frame(x = seq(0,1,.1)), aes(x)) + 
  stat_function(fun=function(x) dbeta(x, 1, 1)) +
  theme_bw() +
  labs(
    title = expression(
      paste(
        "Prior density on the mean of ability parameters: ",
        alpha[.]/(alpha[.] + beta[.])%~%f[Beta](1,1)
      ))
    , y = "Density"
    , x = expression(alpha[.]/(alpha[.] + beta[.]))
  )

ggplot(data.frame(x = 1:20), aes(x)) + 
  stat_function(fun=function(x) extraDistr::dpareto(x, 1.5, 1)) +
  theme_bw() +
  labs(
    title = expression(
      paste(
        "Prior density on the scale of ability parameters: ",
        (alpha[.] + beta[.])%~%f[Pareto](1.5,1)
      ))
    , y = "Density"
    , x = expression((alpha[.] + beta[.]))
  )

```

If we sample 1000 pairs from these distributions and draw from Beta distributions with corresponding parameters, we get the following empirical distribution:

```{r inspect beta hyperprior distr}
beta_means <- rbeta(1000, 1, 1)
beta_scales <- extraDistr::rpareto(1000, 1.5, 1)

# we take advantage of vectorization in R
alphas <- beta_means*beta_scales
betas <- beta_scales-alphas
x <- rbeta(1000, alphas, betas)
```

```{r plot beta hyperprior distr, echo = FALSE}
ggplot(tibble(x = x), aes(x = x)) +
  geom_histogram(alpha = .75) +
  theme_bw() + 
  labs(
    title = expression(paste("Distribution of 1000 sampled values of ", pi[t]%~%f[Beta](alpha, beta)))
    , subtitle = expression(
      paste( 
        " with mean ",
        alpha/(alpha+beta)%~%f[Beta](1,1),
        " and scale ",
        (alpha+beta)%~%f[Pareto](1.5,1)
      )
    )
    , x = expression(pi[t])
    , y = "Count"
  )
```

Essentially, our prior is pushing coders' ability parameters to the extremes.
This seems to be not a very wise choice, since it reflects a belief that coders are either very capable or mal-intended/adversarial (i.e., judgments negatively correlated to true classes), with mediocre and slightly negatively correlated coders being rare.


### Estimation

Befor fitting the models, we need to create a list containing all the data required by the model.
```{r create JAGS model data}

# data
codings_data <- list() 

# number of items
codings_data$N <- length(unique(sim_codings$item))
# number of coders
codings_data$M <- length(unique(sim_codings$coder))
# number of judgements
codings_data$J <- length(sim_codings$judgment)
# judgement data
codings_data$y <- sim_codings %>% 
  arrange(item, coder) %>% 
  # columns: item ID, coder ID, judgement
  select(item, coder, judgment)

```


#### Baseline

As a baseline fit, I obtained 1000 iterations from 2 chains with 500 burn-in iterations.

```{r fit baseline model, eval = FALSE}
model_file_path <- "./models/beta-binomial_by_annotator.jags"

# for obtaining Deviance Information Criterion (DIC)
load.module("dic")

# initialize model with 3 chains
basline_model <- jags.model(
  file = model_file_path
  , data = codings_data
  , n.chains = 3
)

# burn-in for 500 iterations
update(basline_model, 500)

# fit model for 1000 iterations
basline_fit <- coda.samples(
  basline_model
  , variable.names = c(
    # include DIC
    "deviance"
    # prevalence
    , "pi"
    # classes
    , "c"
    # coder ability parameters
    , "theta0", "theta1"
    # hyperprior governing Beta prior on coder sensitivities 
    , "alpha0", "beta0"
    # hyperprior governing Beta prior on coder specificities 
    , "alpha1", "beta1"
  )
  # 100 iterations
  , n.iter = 1000
  # no thinning
  , thin = 1
)
```

```{r load baseline model, include = FALSE, warning=TRUE}
# save to disk
blm <- readRDS("./fits/betabinom_by_annotator_baseline.RData")
baseline_fit <- blm$fit
```

updating and fitting the model took `r blm$runtime` minutes on my MacBook Pro with a 3.5 GHz Intel Core i7 Processor using one core.

##### Inspect fit ---- 

First we want to see whether chains mix and the model generally converged. 
Due to the abundance of parameters in our model, we use the deviance information criterion (DIC) to assess these questions.
```{r plot baseline deviance}
bl_dic <- get_mcmc_estimates(fit.obj = baseline_fit, params = "deviance")

bl_dic %>% 
  ggplot(aes(x = iter, y = est, color = factor(chain))) +
  geom_line(alpha = .5) +
  theme_bw() +
  labs(
    title = "Deviance information criterion (DIC) for first 1000 iterations"
    , subtile = "Obtained for baseline model specification"
    , y = "DIC"
    , x = "Iteration"
    , color = "Chains:"
  ) +
  theme(
    legend.position = "bottom"
  )

bl_dic %>% 
  ggplot(aes(x = est)) + 
  geom_density(fill = "grey", alpha = .75, color = NA) + 
  theme_bw() +
  labs(
    title = "Distribution of deviance information criterion after burn-in"
    , subtitle = "Obtained from 3 chains for 1000 iterations"
    , x = "DIC"
    , y = "Density"
  )
```

```{r gelman.plot baseline deviance}
coda::gelman.plot(baseline_fit[, "deviance"])
```

As we can see, the model nicely converged already after only a few of iterations.

We alos find only very limited interation in the first and second iteration, so there is apparently no need fro thinning.
```{r inspect baseline autocorr}
coda::autocorr.plot(baseline_fit[, "deviance"], ask = F)
```

##### Inspect baseline parameter estimates

###### Prevalence

Chains mix nicely, and the model converges quickely
```{r plot baseline pi}
bl_pi <- get_mcmc_estimates(fit.obj = baseline_fit, params = "pi")

bl_pi %>% 
  ggplot(aes(x = iter, y = est, color = factor(chain))) +
  geom_line(alpha = .5) +
  theme_bw() +
  labs(
    title = expression(paste("Posterior estimates of ", pi, " for first 1000 iterations"))
    , subtitle = "Obtained for baseline model specification"
    , y = expression(pi)
    , x = "Iteration"
    , color = "Chains:"
  ) +
  theme(
    legend.position = "bottom"
    , axis.title.y = element_text(angle = 0, vjust = .5)
  )

coda::gelman.plot(baseline_fit[, "pi"], main = expression(pi))
```

But posterior density has mass in region $[.78, .81]$!
```{r plot baseline mpdf pi}
bl_pi %>% 
  ggplot(aes(x = est)) + 
  geom_density(fill = "grey", alpha = .75, color = NA) + 
  scale_x_continuous(limits = c(0,1)) +
  geom_vline(xintercept = pi, color = "red") +
  theme_bw() +
  labs(
    title = expression(paste("Distribution of posterior estimates of ", pi, " and simulated parameter value"))
    , subtitle = "Red vertical line marks simulated parameter value"
    , caption = "Obtained from 3 chains for 1000 iterations"
    , x = expression(pi)
    , y = "Density"
  ) +
  theme(plot.caption = element_text(hjust = 0))
```


###### Coder ability parameters

Since we have `r n_coders` posterior density distributions for each $\theta_0, \theta_1$, we'll omit visual inspection of chains and autocorrelation and instead rely on the DIC to judge convergence.

Instead we directly plot marginal posterior densities by coder and parameter:
```{r plot baseline mpdf thetas, echo = FALSE, message = FALSE}
post_thetas <- get_mcmc_estimates(fit.obj = baseline_fit, params = "^theta(0|1)\\[\\d+\\]$")

post_thetas %>% 
  separate(parameter, c("parameter", "coder"), sep = "\\[") %>% 
  mutate(
    coder = gsub("\\]$", "", coder)
    # make use of regex capturing grpoup operator (https://stackoverflow.com/a/48365518)
    , parameter = gsub("(\\d)$", "[\\1]", parameter, perl = TRUE)
  ) %>% 
  ggplot(
    aes(
      y = factor(coder, levels = 1:n_coders)
      , x = est
      , group = coder
    )
  ) +
  geom_density_ridges(
    scale = .5
    , color = NA
  ) +
  geom_point(
    shape = 3
    , color = "red"
    , data = tibble(
      coder = rep(factor(1:n_coders, levels = 1:n_coders), 2)
      , parameter = rep(paste0("theta[", 0:1, "]"), each = n_coders)
      , value = c(sim_theta0, sim_theta1)
    )
    , aes(y = factor(coder), x = value, group = coder)
  ) +
  scale_x_continuous(limits = c(0,1))+
  facet_grid(
    rows = vars(parameter)
    , labeller = label_parsed
    , switch = "y"
  ) + 
  labs(
    title = "Marginal posterior densities of ability parameters by coder"
    , subtitle = "Red crosses mark simulated parameter values"
    , x = ""
    # , y = "Coder"
    , y = ""
    , fill = "Parameter"
    , caption = expression(paste(
      theta[0]," := specificity (true-negative rate); ",
      theta[1]," := sensitivity (true-positive rate)"
    ))
  ) + 
  coord_flip() +
  theme_bw() +
  theme(
    strip.text.y = element_text(angle = 180)
    # , strip.background = element_rect(fill = NA)
    , plot.caption = element_text(hjust = 0)
  )
```

Just as in the case of prevalence, we can clearly see how ability parameter estimates are consistently flipped relative to the 'true' simulation parameter values.

In fact, estimates and true values are negatively correlated as the next plot illustrates:
```{r plot baseline thetas corr}
post_thetas %>% 
  separate(parameter, c("parameter", "coder"), sep = "\\[") %>% 
  mutate(
    coder = as.integer(gsub("\\]$", "", coder))
    # make use of regex capturing grpoup operator (https://stackoverflow.com/a/48365518)
    , parameter = gsub("(\\d)$", "[\\1]", parameter, perl = TRUE)
  ) %>% 
  group_by(coder, parameter) %>% 
  summarise(
    mean = mean(est)
    , q5 = quantile(est, .05)
    , q95 = quantile(est, .95)
  ) %>% 
  arrange(parameter, coder) %>% 
  ungroup() %>% 
  mutate(true_val = c(sim_theta0, sim_theta1)) %>% 
  ggplot(aes(x = true_val, y = mean)) +
    # geom_smooth(method='lm', formula=y~x, se = FALSE) +
    geom_point(size = .5) +
    geom_linerange(aes(ymin = q5, ymax = q95), width = .1) +
    scale_x_continuous(limits = 0:1) +
    scale_y_continuous(limits = 0:1) +
    geom_abline(slope = 1, intercept = 0, size = .5, color = "grey") +
    facet_grid(~parameter, labeller = label_parsed) +
    labs(
      title = "Mean posterior coder abilities vs. simulated parameter values"
      , subtitle = "Points depict mean posterior values, overlaying vertical lines 90% credibility intervalls"
      , x = "Simulated parameter value"
      , y = "Posterior value"
      , caption = "diagonal indicates perfect correspondence"
    ) +
    theme_bw() + 
    theme(plot.caption = element_text(hjust = 0))
```

The data pulls ability parameters towards low values, but the informative beta priors pull in the opposite direction.
We can also see how the more informative (more dispersed) prior on $\theta_0$ ($f_{B}(40,8)$ results in higher dispersion of posteriors.

###### Item classes 

Finally, we can take a look on the model-classification of items relative to their true simulated values.

```{r inspect baseline class membership}
# gives n_items*n_chains*n_iterations = 1000*3*1000 = 3,000,000 rows
post_c <- get_mcmc_estimates(fit.obj = baseline_fit, params = "c\\[\\d+\\]")

post_c %>% 
  group_by(parameter) %>% 
  summarise(pr_pos = sum(est)/n()) %>% 
  mutate(
    item = as.integer(stringr::str_extract(parameter, "\\d+"))
  ) %>% 
  mutate(est_class = as.integer(pr_pos > .5)) %>% 
  left_join(tibble(item = 1:n_items, true_class = sim_item_classes)) %>% 
  mutate(aggreement = true_class == est_class) %>% 
  summarise(accuracy = sum(aggreement)/n())
```

Model-based classifications of items are almost perfectly contradicting true class memberships.

###### Discussion

In sum, evaluation of the baseline model suggests that we would want to constraint the model to induce convergence on true values and not on mirroring ones.

Alternatively, post-hoc adjustment may be an option.
However, in contrast to classical item response models, we cannot standardize latent class estimates since they follow a Bernoulli distribution.

Also, items' classes are outside of the context of simulated data unknown, so inducing 'correct' convergence 

#### Inducing correct convergence via the prior on $\pi$

I also tried to induce correct convergence by using the following priors on $\pi$.

```{r plot alt pi priors}
ggplot(data.frame(x = seq(0,1,.1)), aes(x)) + 
  stat_function(fun=function(x) dbeta(x*2, 2, 2), aes(color = "2,2")) +
  stat_function(fun=function(x) dbeta(x*2, 1.5, 1.5), aes(color = "1.5,1.5")) +
  stat_function(fun=function(x) dbeta(x*2, 1.1, 1.1), aes(color = "1.1,1.1")) +
  labs(
    title = expression(paste("Alternative prior densities of ", pi))
    , y = "Density"
    , x = expression(pi)
    , color = ""
  ) + 
  theme_bw() +
  theme(legend.position = "top") +
  scale_colour_discrete(
    labels = c(
      expression(pi%~%f[B](2,2)/2)
      , expression(pi%~%f[B](1.5,1.5)/2)
      , expression(pi%~%f[B](1.1,1.1)/2)
    )
  )

```

I implemented this in the JAGS code for the second prior density by replacing the line `pi ~ dbeta(1,1)` by the following code
```
pi_temp ~ dbeta(1.5, 1.5);
pi <- pi_temp/2;
```

The idea here is that if $\pi$ is constrained to range between $[0, .5]$ *a priori*, the data could not pull the estimate in the range $(.5, 1]$, and as item classes are drawn from $\text{Bernoulli}(\pi)$, this would induce convergence on the correct parameter values.
This did not work as intended, however.

#### Flat prior on $\theta_0,\theta_1$ and specifying initializing values close to the true parameter of $\pi$

Next, I wondered wether the problem resulted from the weird-shaped prior distribution generated by the hyperpriors $(\alpha_0, \beta_0)$ and $(\alpha_1, \beta_1)$.
I thus discarded the multilevel component of the model and simply gave coders' ability parameters flat priors on the interval [0,1] using a Beta distribution with $a = b = 1$.

This adapted model then was specified as follows:
```
model{
    for (i in 1:N){
        c[i] ~ dbern(pi)
    }

    for (j in 1:M) {
        theta0[j]~ dbeta(alpha0, beta0);
        theta1[j]~ dbeta(alpha1, beta1);
    }

    for (j in 1:J) {
        y[j,3] ~ dbern(c[y[j,1]]*theta1[y[j,2]]+(1-c[y[j,1]])*(1-theta0[y[j,2]]))
    }

    pi ~ dbeta(1,1);
    
    alpha0 <- 1;
    beta0 <- 1;

    alpha1 <- 1;
    beta1 <- 1;
}
```

However, the results of this exercise were mixed: one of two chains would oscilate around .8, and the other two around .2, or vice versa.

#### Flat prior on $\theta_0,\theta_1$, informative prior for $\pi$, and specifying initializing values close to the true parameter of $\pi$

Next, instead of a flar prior on $\pi$, then I used an informative prior using a Beta distribution with $a = 1.5, b = 7$. 

Thus, this simplified adapted model has the following specification:

$$
\begin{align*}
c_i &\sim\ \mbox{Bernoulli}(\pi)\\
\theta_{0j} &\sim\ \mbox{Beta}(1 , 1)\\
\theta_{1j} &\sim\ \mbox{Beta}(1 , 1)\\
y_{ij} &\sim\ \mbox{Bernoulli}(c_i\theta_{1j} + (1 - c_i)(1  - \theta_{0j}))\\
{}&{}\\
\pi &\sim\ \mbox{Beta}(1.5,7)\\
\end{align*}
$$




That is, I changed the line `pi ~ dbeta(1,1)` in the adapted model to `pi ~ dbeta(1.5,7)`.
The prior looks like follows:

```{r informative beta prior}
ggplot(data.frame(x = seq(0,1,.1)), aes(x)) + 
  stat_function(fun=function(x) dbeta(x*2, 1.5, 7)) +
  labs(
    title = expression(paste("Prior density ", pi%~%f[Beta](1.5,7), " in adapted model"))
    , y = "Density"
    , x = expression(pi)
  ) + 
  theme_bw()
```

This was the only possible way to obtain correct classifications and posterior densities to center on or around to the simulated values.




```{r fit adapted model, eval = FALSE}
init_vals <- lapply(1:3, function(chain) {

  out <- list("pi" = rbeta(1, 1, 8))
  out[[".RNG.name"]] <- "base::Wichmann-Hill"
  out[[".RNG.seed"]] <- 1234
  
  return(out)
})

# load model
model_file_path <- "./models/beta-binomial_by_annotator_adapted.jags"

adapted_model <- jags.model(
  file = model_file_path
  , data = codings_data
  , inits = init_vals
  , n.chains = 3
)

update(adapted_model, 500)

adapted_fit <- coda.samples(
  adapted_model
  , variable.names = c(
    "deviance"
    , "pi"
    , "c"
    , "theta0", "theta1"
  )
  , n.iter = 1000
  , thin = 1
)
```

```{r load adapted fit, include = FALSE}
ad <- readRDS("./fits/betabinom_by_annotator_adapted.RData")
adapted_fit <- ad$fit
```


##### Inspect fit 

First we want to see whether chains mix and the model generally converged. 
We again confie visual insteopction to the DIC.

```{r plot adapted deviance, echo = FALSE}
bl_dic <- get_mcmc_estimates(fit.obj = adapted_fit, params = "deviance")

bl_dic %>% 
  ggplot(aes(x = iter, y = est, color = factor(chain))) +
  geom_line(alpha = .5) +
  theme_bw() +
  labs(
    title = "Deviance information criterion (DIC) for first 1000 iterations"
    , subtitle = "Obtained for adapted model specification"
    , y = "DIC"
    , x = "Iteration"
    , color = "Chains:"
  ) +
  theme(
    legend.position = "bottom"
  )

bl_dic %>% 
  ggplot(aes(x = est)) + 
  geom_density(fill = "grey", alpha = .75, color = NA) + 
  theme_bw() +
  labs(
    title = "Distribution of deviance information criterion after burn-in"
    , subtitle = "Obtained from 3 chains for 1000 iterations"
    , x = "DIC"
    , y = "Density"
  )
```

```{r gelman.plot adapted deviance, echo = FALSE}
coda::gelman.plot(adapted_fit[, "deviance"])
```

Again, the model nicely converges, and very limited autocorrelation in the first and second iteration.
```{r plot adapted autocorr, echo = FALSE}
coda::autocorr.plot(adapted_fit[, "deviance"], ask = F)
```

##### Inspect adapted parameter estimates

###### Prevalence

Chains mix nicely, and the model converges quickely.
```{r plot adapted pi, echo = FALSE}
bl_pi <- get_mcmc_estimates(fit.obj = adapted_fit, params = "pi")

bl_pi %>% 
  ggplot(aes(x = iter, y = est, color = factor(chain))) +
  geom_line(alpha = .5) +
  theme_bw() +
  labs(
    title = expression(paste("Posterior estimates of ", pi, " for first 1000 iterations"))
    , subtitle = "Obtained for baseline model specification"
    , y = expression(pi)
    , x = "Iteration"
    , color = "Chains:"
  ) +
  theme(
    legend.position = "bottom"
    , axis.title.y = element_text(angle = 0, vjust = .5)
  )

coda::gelman.plot(adapted_fit[, "pi"], main = expression(pi))
```

And it does so on the simulatred value:
```{r plot adapted mpdf pi, echo = FALSE}
bl_pi %>% 
  ggplot(aes(x = est)) + 
  geom_density(fill = "grey", alpha = .75, color = NA) + 
  scale_x_continuous(limits = c(0,1)) +
  geom_vline(xintercept = pi, color = "red") +
  theme_bw() +
  labs(
    title = expression(paste("Distribution of posterior estimates of ", pi, " and simulated parameter value"))
    , subtitle = "Red vertical line marks simulated parameter value"
    , caption = "Obtained from 3 chains for 1000 iterations"
    , x = expression(pi)
    , y = "Density"
  ) +
  theme(plot.caption = element_text(hjust = 0))
```


###### Coder ability parameters

Inspecting the marginal posterior densities of ability parameters by coder and parameter, we also find that the adapted model by and larfge converges on the simulated parameters
```{r plot adapted mpdf thetas, echo = FALSE}
post_thetas <- get_mcmc_estimates(fit.obj = adapted_fit, params = "^theta(0|1)\\[\\d+\\]$")

post_thetas %>% 
  separate(parameter, c("parameter", "coder"), sep = "\\[") %>% 
  mutate(
    coder = gsub("\\]$", "", coder)
    # make use of regex capturing grpoup operator (https://stackoverflow.com/a/48365518)
    , parameter = gsub("(\\d)$", "[\\1]", parameter, perl = TRUE)
  ) %>% 
  ggplot(
    aes(
      y = factor(coder, levels = 1:n_coders)
      , x = est
      , group = coder
    )
  ) +
  geom_density_ridges(
    scale = .5
    , color = NA
  ) +
  geom_point(
    shape = 3
    , color = "red"
    , data = tibble(
      coder = rep(factor(1:n_coders, levels = 1:n_coders), 2)
      , parameter = rep(paste0("theta[", 0:1, "]"), each = n_coders)
      , value = c(sim_theta0, sim_theta1)
    )
    , aes(y = factor(coder), x = value, group = coder)
  ) +
  scale_x_continuous(limits = c(0,1))+
  facet_grid(
    rows = vars(parameter)
    , labeller = label_parsed
    , switch = "y"
  ) + 
  labs(
    title = "Marginal posterior densities of ability parameters by coder"
    , subtitle = "Red crosses mark simulated parameter values"
    , x = ""
    # , y = "Coder"
    , y = ""
    , fill = "Parameter"
    , caption = expression(paste(
      theta[0]," := specificity (true-negative rate); ",
      theta[1]," := sensitivity (true-positive rate)"
    ))
  ) + 
  coord_flip() +
  theme_bw() +
  theme(
    strip.text.y = element_text(angle = 180)
    # , strip.background = element_rect(fill = NA)
    , plot.caption = element_text(hjust = 0)
  )

```

For the fit obtained by the adapted model, estimates and true values are now positively:
```{r plot adapted thetas corr, echo = FALSE}

post_thetas %>% 
  separate(parameter, c("parameter", "coder"), sep = "\\[") %>% 
  mutate(
    coder = as.integer(gsub("\\]$", "", coder))
    # make use of regex capturing grpoup operator (https://stackoverflow.com/a/48365518)
    , parameter = gsub("(\\d)$", "[\\1]", parameter, perl = TRUE)
  ) %>% 
  group_by(coder, parameter) %>% 
  summarise(
    mean = mean(est)
    , q5 = quantile(est, .05)
    , q95 = quantile(est, .95)
  ) %>% 
  arrange(parameter, coder) %>% 
  ungroup() %>% 
  mutate(true_val = c(sim_theta0, sim_theta1)) %>% 
  ggplot(aes(x = true_val, y = mean)) +
  # geom_smooth(method='lm', formula=y~x, se = FALSE) +
  geom_point(size = .5) +
  geom_linerange(aes(ymin = q5, ymax = q95), width = .1) +
  scale_x_continuous(limits = 0:1) +
  scale_y_continuous(limits = 0:1) +
  geom_abline(slope = 1, intercept = 0, size = .5, color = "grey") +
  facet_grid(~parameter, labeller = label_parsed) +
  labs(
    title = "Mean posterior coder abilities vs. simulated parameter values"
    , subtitle = "Points depict mean posterior values, overlaying vertical lines 90% credibility intervalls"
    , x = "Simulated parameter value"
    , y = "Posterior value"
    , caption = "Diagonal indicates perfect correspondence"
  ) +
  theme_bw() + 
  theme(plot.caption = element_text(hjust = 0))
```

###### Item classes 

Finally, we can take a look on the model-classification of items relative to their true simulated values.

```{r inspect adapted class membership}
# gives n_items*n_chains*n_iterations = 1000*3*1000 = 3,000,000 rows
post_c <- get_mcmc_estimates(fit.obj = adapted_fit, params = "c\\[\\d+\\]")

post_c %>% 
  group_by(parameter) %>% 
  summarise(pr_pos = sum(est)/n()) %>% 
  mutate(
    item = as.integer(stringr::str_extract(parameter, "\\d+"))
  ) %>% 
  mutate(est_class = as.integer(pr_pos > .5)) %>% 
  left_join(tibble(item = 1:n_items, true_class = sim_item_classes)) %>% 
  mutate(aggreement = true_class == est_class) %>% 
  summarise(accuracy = sum(aggreement)/n())
```

The (unadjusted) classification accuracy is almost perfect.

This is slightly better than majority voting:
```{r majortiy voting accuracy}
sim_codings %>% 
  group_by(item) %>% 
  summarise(
    n_pos = sum(judgment)
    , n_neg = sum(!judgment)
    , tie_breaker = rbernoulli(1) 
    , voted = case_when(
      n_pos > n_neg ~ 1L
      , n_pos < n_neg ~ 0L
      , TRUE ~ as.integer(tie_breaker)
    )
    , true_class = unique(true_class)
  ) %>% 
  ungroup() %>% 
  summarise(`majority vote: accuracy` = sum(voted == true_class)/n())
```
